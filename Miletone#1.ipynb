{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 1\n",
    "\n",
    "This is 3 Musketeer group Milestone report. We will present the steps we have done currently, following the structure of the 'Main2.ipynb\" file.\n",
    "\n",
    "We then will outline the steps needed moving forward. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Intial Setup\n",
    "Imported libraries, data as a pandas dataframe, and the data dictionary, as well as created a text file that explains the variables (as notes for ourselves).\n",
    "\n",
    "_Kevin, Martin, Stuart_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Data understanding.\n",
    "\n",
    "- Replaced missing values symobol '-' with NaN. \n",
    "- Looked at number of missing values for each column. \n",
    "- df.describe() on categorical features and numeric features.\n",
    "\n",
    "_Martin and Stuart_\n",
    "\n",
    "Lastly, created a pandas profile object to more detailed overview of our data. Cell 13, filename = 'df_profile_report.html'\n",
    "Here is the code for that. \n",
    "   - #trying out pandas profiling\n",
    "   - #import pandas_profiling as pp\n",
    "   - #pfr = pp.ProfileReport(df)\n",
    "   - #pfr.to_file('df_profile_report')\n",
    "\n",
    "_Kevin_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 3: Analyzing pandas profile report\n",
    "\n",
    "Went through all the results of the report and summerized findings. From these findings, we decide on which steps are needed for purposes of cleaning the dataset.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Going through the variables individually from the report, here are some notes about missing values, extreme values, and strategies to keep in mind moving forward.\n",
    "\n",
    "Starting with the IPO characteristics:\n",
    "- C1: 22 missing values, skewed right. Have to determine appropriate imputation method (ffill, bfill, interpolation?)\n",
    "- C2: 22 missing values, but a dummy/binary variable. can use possible ml (logistic regression) method to impute missing values.\n",
    "- C3: 36 missing values, strong skewness, strong outliers. Will have to smooth this variable's noise. \n",
    "- C4: 22 missing values, fairly normal distribution. Dealing with ratio/returns here, so shouldn't be much noise. Mean/median imputation might be useable\n",
    "- C5: 6 missing values, # shares outstanding, so range is very large; might look like outliers, but can't treat as so since it actually is true. Missing values might actually be able to look up.\n",
    "- C6: 6 mising values, # offering shares, so similar story to C5\n",
    "- C7: __72__ missing values, might be outliers at end range of values, but \"not really\" outliers. Imputing challenge.\n",
    "\n",
    "Looking at the Textual characteristics:\n",
    "- T1: 1 missing value, slightly skewed with some high outliers. Since only 1 missing value, might be appropriate to simply impute with median.\n",
    "- T2: Has minimum value of 0, which is puzzling. Highly correlated with T1 (.93). Would be appropriate to dismiss this variable.\n",
    "- T3: Has minimum value of 0, which is puzzling. Highly correlated with T2 (.95). Also would be appropriate to dismiss thi variable.\n",
    "- T4: Has minimum value of 0, which is puzzling. Highly correlated with T3 (.96). Appropriate to dismiss this variable\n",
    "- T5: 1 missing value, with one very large outlier. Will have to handle this outlier. Has a nonsensical value of -1, need to clean this. \n",
    "\n",
    "Looking at sentiment characterisitics:\n",
    "- S1: 1 missing value, slightly skewed right. Also minimum value of -1, which is nonsensical. Must handle such exceptions\n",
    "- S2: 1 missing value, poisson distribution. Some high outliers. \n",
    "- S3: 1 missing value, skewed right. Fairly \"stable\" variable; not much quirkiness.\n",
    "\n",
    "Looking at IPO Pricing:\n",
    "- P(IPO): 5 missing values, slightly skewed right. This is final IPO offering price, which may be researchable.\n",
    "- P(H): 10 missing values, but contains a value of 0, which wouldn't make sense. Also large outlier needs to be handled.\n",
    "- P(L): Almost perfectly correlating with P(H) (.99). Can be dropped AFTER making target variable calculations. \n",
    "- P(1Day): 22 missing values, also has value of 0. Need to research if values like this and in P(H) make sense. Also two large outliers. \n",
    "\n",
    "Lastly,\n",
    "- I3: 8 missing values, imputing will be need to be done manually by researching the corresponding company.\n",
    "\n",
    "_Kevin_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some variables have nonsensical values, like a value of -1 in T5. Thus, here is a quick summary of any nonsensical values that may arise in our dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Missing Values\n",
    "* Value of -1 in T5 and S1; can't have a negative number in these columns.\n",
    "* Value of 0 in P(1Day); wouldn't make sense for a stock to cost 0$ at end of trading day.\n",
    "* Values of 0 in T2, T3, and T4, which may or may not be a error (could be there was no MD%A). \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 4: Data cleaning.\n",
    "\n",
    "Initially, we started cleaning some of the columns with minimal missing values, such as the 'T' columns.\n",
    "Created box plots for each of the 5 'T' variables to determine which imputation method would be best. Imupted with Median. \n",
    "\n",
    "Repeated process with 'S' columns. Imputed missing values with Medain. \n",
    "\n",
    "_Stuart and Martin_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we started cleaning more compex columns.\n",
    "\n",
    "First column was 'I3'. For missing values, we did research using  https://www.sec.gov/edgar/searchedgar/companysearch.html webesite to manually input correct SIC code for companies with missing values in this column. \n",
    "\n",
    "_Martin_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After cleaning I3, we then cleaned the IPO Pricing characteristics columns. Our intitial strategy was to fill missing values with the mean difference for the total IPO data set.  We filled the p(IPO) with the average IPO for that industry. we then filled P(H) with the overall average difference added to the P(IPO). we did the same for P(L). and for P(1Day) we filled the missing values with the overall average difference between P(1Day) and P(IPO) for the stocks we had data.\n",
    "On review, we now realize that instead we should look at the median and calcualte the median on each sector using the bins we made instead of looking at the average performance of the overall dataset.\n",
    "\n",
    "_Martin and Stuart_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we cleaned the IPO characteristics columns using MICE method. We did it very simplistically (and probably naivly), using a python library called 'impyute'. \n",
    "\n",
    "_Kevin_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: More cleaning\n",
    "To do: fix individual values in the dataset that are nonsensical. First, we focus on values of 0 in the P(IPO) column; we treat these as missing values and will impute a more reasonable value. Grouping by Bins, we will implement the same strategy for the missing values in the same variables above. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Still To Do\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Need to calculate the target variables, as well as the control variables that need to be calculated. \n",
    "\n",
    "Next, we need to normalize and standardize our data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decide what features need to be dropped or engineered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
